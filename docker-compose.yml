services:
  register-worker:
    build:
      context: ./register
      dockerfile: Dockerfile.register_worker
    container_name: byoc-transcription-register-worker
    environment:
      - "ORCH_URL=https://${ORCH_SERVICE_ADDR}"
      - "ORCH_SECRET=${ORCH_SECRET}"
      - "CAPABILITY_NAME=${CAPABILITY_NAME}"
      - "CAPABILITY_DESCRIPTION=video analysis"
      - "CAPABILITY_URL=${AI_RUNNER_URL}"
      - "CAPABILITY_PRICE_PER_UNIT=${CAPABILITY_PRICE_PER_UNIT}"
      - "CAPABILITY_PRICE_SCALING=${CAPABILITY_PRICE_SCALING}"
      - "CAPABILITY_CAPACITY=${CAPABILITY_CAPACITY}"
  transcription:
    image: ${RUNNER_IMAGE}
    build:
      context: .
    container_name: byoc-transcription-1
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ./models:/models
      #- ./src:/app/src
    ports:
      - ${AI_RUNNER_PORT}:8000
  vllm:
    image: vllm/vllm-openai:latest
    container_name: byoc-transcription-vllm
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    volumes:
      - ./models:/root/.cache/huggingface
    ports:
      - 5000:5000
    command: >
      --model ${LOCAL_SUMMARY_MODEL}
      --port 5000
      --gpu-memory-utilization 0.75
      --kv-cache-dtype fp8
      --max-num-batched-tokens 1024
      --max-num-seqs ${MAX_CONCURRENT_SUMMARIES}
networks:
  default:
    name: byoc-runners
    external: true